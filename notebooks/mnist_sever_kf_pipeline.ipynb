{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "!pip install tensorflow numpy kfp boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ, path\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    # allow s3 connection without creds\n",
    "    if \"AWS_ACCESS_KEY_ID\" not in environ:\n",
    "        from botocore import UNSIGNED\n",
    "        from botocore.client import Config\n",
    "\n",
    "        return boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "    return boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def download_s3(bucket, key, outdir):\n",
    "    s3 = get_s3_client()\n",
    "    s3_object = s3.get_object(Bucket=bucket, Key=key)\n",
    "    stream = s3_object[\"Body\"]\n",
    "    outfile = path.join(outdir, key)\n",
    "    filepath = path.abspath(outfile)\n",
    "    parent_dir = path.dirname(outfile)\n",
    "    Path(parent_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(outfile, \"wb+\") as f:\n",
    "        f.write(stream.read())\n",
    "    print(f\"file saved to: {outfile}\")\n",
    "\n",
    "\n",
    "def parse_s3_url(url):\n",
    "    print(f\"downloading: {url}\")\n",
    "    u = urlparse(url)\n",
    "    bucket = u.netloc.split(\".\")[0]\n",
    "    key = u.path.strip(\"/\")\n",
    "    return bucket, key\n",
    "\n",
    "\n",
    "def download_s3_dir(data_urls: List[str], data_dir: str):\n",
    "    \"\"\"Download objects from S3\"\"\"\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    for data_url in data_urls:\n",
    "        bucket, key = parse_s3_url(data_url)\n",
    "        download_s3(bucket, key, data_dir)\n",
    "\n",
    "\n",
    "def download_data(data_dir: str, data_urls: str):\n",
    "    # data_urls must be type string because kubeflow has no registered serializers for type \"typing.List[str]\"\n",
    "    download_s3_dir(data_urls.split(\",\"), data_dir)\n",
    "    print(\"downloads complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igI1dvp7SWWV"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def gen_metadata(tensorboard_log_dir):\n",
    "    return {\n",
    "        \"outputs\": [\n",
    "            {\n",
    "                \"type\": \"tensorboard\",\n",
    "                \"source\": tensorboard_log_dir,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def write_metadata(\n",
    "    metadata_file: str,\n",
    "    bucket: str,\n",
    "    bucket_dir: str,\n",
    "):\n",
    "    tensorboard_log_dir = f\"s3://{bucket}/{bucket_dir}\"\n",
    "    with open(metadata_file, \"w\") as f:\n",
    "        json.dump(gen_metadata(tensorboard_log_dir), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    # allow s3 connection without creds\n",
    "    if \"AWS_ACCESS_KEY_ID\" not in os.environ:\n",
    "        from botocore import UNSIGNED\n",
    "        from botocore.client import Config\n",
    "\n",
    "        return boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "    return boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def upload_file(s3_client, file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    return s3_client.upload_file(file_name, bucket, object_name)\n",
    "\n",
    "\n",
    "def bucket_exists(s3_client, bucket_name):\n",
    "    exists = True\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "    except ClientError as e:\n",
    "        # If a client error is thrown, then check that it was a 404 error.\n",
    "        # If it was a 404 error, then the bucket does not exist.\n",
    "        error_code = e.response[\"Error\"][\"Code\"]\n",
    "        if error_code == \"404\":\n",
    "            exists = False\n",
    "    return exists\n",
    "\n",
    "\n",
    "def upload_dir(\n",
    "    src_dir: str,\n",
    "    bucket_name: str,\n",
    "    bucket_dir: str,\n",
    "):\n",
    "    s3_client = get_s3_client()\n",
    "\n",
    "    if not bucket_exists(s3_client, bucket_name):\n",
    "        raise Exception(f\"Bucket: {bucket_name} does not exist\")\n",
    "\n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for name in files:\n",
    "            local_path = os.path.join(root, name)\n",
    "            upload_file(\n",
    "                s3_client,\n",
    "                local_path,\n",
    "                bucket_name,\n",
    "                f\"{bucket_dir}/{os.path.relpath(local_path, src_dir)}\",\n",
    "            )\n",
    "\n",
    "    response = s3_client.list_objects(Bucket=bucket_name)\n",
    "    print(f\"All objects in {bucket_name}:\")\n",
    "\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(f\"{bucket_name}/{file['Key']}\")\n",
    "\n",
    "\n",
    "# upload_dir(src_dir, bucket_name, bucket_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def gen_log_dirname(log_dir) -> str:\n",
    "    return path.join(log_dir, \"tensorboard\", \"fit\")\n",
    "\n",
    "\n",
    "def load_mnist(filepath, kind, normalize=True):\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `filepath`\"\"\"\n",
    "    labels_path = path.join(filepath, f\"{kind}-labels-idx1-ubyte.gz\")\n",
    "    images_path = path.join(filepath, f\"{kind}-images-idx3-ubyte.gz\")\n",
    "\n",
    "    with gzip.open(labels_path, \"rb\") as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path, \"rb\") as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(\n",
    "            len(labels), 28, 28\n",
    "        )\n",
    "\n",
    "    # normalize by dividing each pixel value by 255.0. This places the pixel value within the range 0 and 1.\n",
    "    if normalize:\n",
    "        images = images / 255.0\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def learning_rate(batch_size):\n",
    "    # gradually reduce the learning rate during training\n",
    "    return keras.optimizers.schedules.InverseTimeDecay(\n",
    "        0.001, decay_steps=batch_size * 1000, decay_rate=1, staircase=False\n",
    "    )\n",
    "\n",
    "\n",
    "def create_model(batch_size):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            keras.layers.Dense(128, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(128, activation=\"relu\"),\n",
    "            keras.layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            # learning_rate=1e-3,\n",
    "            learning_rate=learning_rate(batch_size),\n",
    "        ),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_dir: str,\n",
    "    data_dir: str,\n",
    "    log_dir: str,\n",
    "    epochs: int = 5,\n",
    ") -> str:  # noqa: F821\n",
    "    \"\"\"Trains a model and saves to model dir and returns path to tensorboard logs.\"\"\"\n",
    "\n",
    "    train_images, train_labels = load_mnist(data_dir, kind=\"train\", normalize=True)\n",
    "    test_images, test_labels = load_mnist(data_dir, kind=\"t10k\", normalize=True)\n",
    "\n",
    "    tensorboard_log_dir = gen_log_dirname(log_dir)\n",
    "\n",
    "    batch_size = len(train_images)\n",
    "    model = create_model(batch_size)\n",
    "    model.fit(\n",
    "        x=train_images,\n",
    "        y=train_labels,\n",
    "        epochs=epochs,\n",
    "        shuffle=True,\n",
    "        # tensorboard args\n",
    "        validation_data=(test_images, test_labels),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=tensorboard_log_dir, histogram_freq=1\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model.save(model_dir, include_optimizer=True)\n",
    "    return tensorboard_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_upload_tensorboard_s3(\n",
    "    outdir,\n",
    "    datadir,\n",
    "    logdir,\n",
    "    epochs,\n",
    "    bucket,\n",
    "    bucketdir,\n",
    "    metadatafile,\n",
    "):\n",
    "    tensorboard_log_dir = train_model(\n",
    "        outdir,\n",
    "        datadir,\n",
    "        logdir,\n",
    "        epochs,\n",
    "    )\n",
    "\n",
    "    # upload tensorboard logs to s3\n",
    "    upload_dir(tensorboard_log_dir, bucket, bucketdir)\n",
    "\n",
    "    # write metdata file pointing to tensorboard logs hosted on S3\n",
    "    write_metadata(metadatafile, bucket, bucketdir)\n",
    "\n",
    "    return tensorboard_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pxGKEae6Eo_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import numpy as np\n",
    "\n",
    "from typing import NamedTuple\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "def write_cm_to_csv(cm, class_labels, cm_path):\n",
    "    data = []\n",
    "    for target_index, target_row in enumerate(cm):\n",
    "        for predicted_index, count in enumerate(target_row):\n",
    "            data.append(\n",
    "                (class_labels[target_index], class_labels[predicted_index], count)\n",
    "            )\n",
    "\n",
    "    df_cm = pd.DataFrame(data, columns=[\"target\", \"predicted\", \"count\"])\n",
    "    with file_io.FileIO(cm_path, \"w\") as f:\n",
    "        df_cm.to_csv(\n",
    "            f, columns=[\"target\", \"predicted\", \"count\"], header=False, index=False\n",
    "        )\n",
    "\n",
    "\n",
    "def predict(model, test_images):\n",
    "    # Define a Softmax layer to define outputs as probabilities\n",
    "    probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "    predictions = probability_model.predict(test_images)\n",
    "    return np.ravel(np.matrix(predictions).argmax(1))\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    metrics_path: str,\n",
    "    data_dir: str,\n",
    "    model_dir: str,\n",
    ") -> NamedTuple(\n",
    "    \"output\",\n",
    "    # https://www.kubeflow.org/docs/pipelines/sdk/pipelines-metrics/\n",
    "    # The output name must be MLPipeline Metrics or MLPipeline_Metrics (case does not matter).\n",
    "    [(\"mlpipeline_ui_metadata\", \"UI_metadata\"), (\"mlpipeline_metrics\", \"Metrics\")],\n",
    "):\n",
    "\n",
    "    test_images, test_labels = load_mnist(data_dir, kind=\"t10k\", normalize=False)\n",
    "    model = keras.models.load_model(model_dir)\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "    metrics = {\n",
    "        \"metrics\": [\n",
    "            {\"name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n",
    "            {\"name\": \"accuracy\", \"numberValue\": str(accuracy), \"format\": \"PERCENTAGE\"},\n",
    "        ]\n",
    "    }\n",
    "    with open(metrics_path, \"w+\") as f:\n",
    "        json.dump(metrics, f)\n",
    "\n",
    "    print_output = namedtuple(\n",
    "        # \"pipeline_metrics\" is hardcoded value that could be anything\n",
    "        \"output\",\n",
    "        [\"pipeline_metrics\"],\n",
    "    )\n",
    "    return print_output(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ, path\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "from kfp.components import InputPath, OutputPath\n",
    "\n",
    "\n",
    "def get_s3_client():\n",
    "    # allow s3 connection without creds\n",
    "    if \"AWS_ACCESS_KEY_ID\" not in environ:\n",
    "        from botocore import UNSIGNED\n",
    "        from botocore.client import Config\n",
    "\n",
    "        return boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "    return boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def upload_file(s3_client, file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    return s3_client.upload_file(file_name, bucket, object_name)\n",
    "\n",
    "\n",
    "def bucket_exists(s3_client, bucket_name):\n",
    "    exists = True\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket_name)\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        # If a client error is thrown, then check that it was a 404 error.\n",
    "        # If it was a 404 error, then the bucket does not exist.\n",
    "        error_code = e.response[\"Error\"][\"Code\"]\n",
    "        if error_code == \"404\":\n",
    "            exists = False\n",
    "    return exists\n",
    "\n",
    "\n",
    "def export_model(\n",
    "    src_dir: str,\n",
    "    bucket_name: str,\n",
    "    bucket_dir: str,\n",
    "    # model_dir: InputPath(str), bucket_name: str, bucket_dir: str,\n",
    "):\n",
    "    s3_client = get_s3_client()\n",
    "\n",
    "    if not bucket_exists(s3_client, bucket_name):\n",
    "        raise Exception(f\"Bucket: {bucket_name} does not exist\")\n",
    "\n",
    "    for root, dirs, files in os.walk(model_dir):\n",
    "        for name in files:\n",
    "            local_path = os.path.join(root, name)\n",
    "            upload_file(\n",
    "                s3_client,\n",
    "                local_path,\n",
    "                bucket_name,\n",
    "                f\"{bucket_dir}/{os.path.relpath(local_path, model_dir)}\",\n",
    "            )\n",
    "\n",
    "    response = s3_client.list_objects(Bucket=bucket_name)\n",
    "    print(f\"All objects in {bucket_name}:\")\n",
    "\n",
    "    for file in response[\"Contents\"]:\n",
    "        print(f\"{bucket_name}/{file['Key']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as components\n",
    "from kfp import dsl, Client\n",
    "from kfp.aws import use_aws_secret\n",
    "import os\n",
    "\n",
    "repo = \"985486441319.dkr.ecr.eu-west-2.amazonaws.com/example-kubeflow-app/components\"\n",
    "BOTO_IMAGE = f\"{repo}/boto:9c8763365c27682acac4514eafac52ebe829e72c\"\n",
    "TF_IMAGE = f\"{repo}/tensorflow:62d0ce59ebc14a6f42cf23fd7180dcad129da1de\"\n",
    "\n",
    "\n",
    "def mnist_pipeline(  # nosec\n",
    "    aws_secret_name: str = \"aws-s3-data-secret-kfaas-demo\",\n",
    "    model_name: str = \"mnist-fashion\",\n",
    "    model_version: str = \"1\",\n",
    "    epochs: int = 10,\n",
    "    bucket: str = \"kfaas-demo-data-sandbox\",\n",
    "    bucket_dir_model: str = \"demo/models\",\n",
    "    bucket_dir_tensorboard: str = \"demo/tensorboard\",\n",
    "):\n",
    "\n",
    "    metadata_file = os.path.join(\"tmp\", \"metadata.json\")\n",
    "    metrics_path = os.path.join(\"tmp\", \"metrics.json\")\n",
    "    log_dir = os.path.join(\"tmp\", \"logs\")\n",
    "\n",
    "    mnt_path = \"/mnt\"\n",
    "    datasets_dir = \"/mnt/datasets\"\n",
    "    model_dir = \"/mnt/model\"\n",
    "    mnist_data_s3_urls = [\n",
    "        \"https://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\",\n",
    "        \"https://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\",\n",
    "        \"https://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\",\n",
    "        \"https://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\",\n",
    "    ]\n",
    "\n",
    "    aws_secret = use_aws_secret(\n",
    "        aws_secret_name, \"AWS_ACCESS_KEY_ID\", \"AWS_SECRET_ACCESS_KEY\"\n",
    "    )\n",
    "\n",
    "    vop = dsl.VolumeOp(\n",
    "        name=\"create_volume\",\n",
    "        resource_name=\"data-volume\",\n",
    "        size=\"1Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO,\n",
    "    )\n",
    "\n",
    "    download_op = components.func_to_container_op(\n",
    "        download_data,\n",
    "        base_image=BOTO_IMAGE,\n",
    "        output_component_file=\"download_component.yaml\",\n",
    "    )\n",
    "    download_task = download_op(\n",
    "        data_dir=datasets_dir,\n",
    "        data_urls=(\",\").join(mnist_data_s3_urls),\n",
    "    ).add_pvolumes({mnt_path: vop.volume})\n",
    "\n",
    "    train_op = components.func_to_container_op(\n",
    "        train_upload_tensorboard_s3,\n",
    "        base_image=TF_IMAGE,\n",
    "        output_component_file=\"train_component.yaml\",\n",
    "    )\n",
    "\n",
    "    train_task = (\n",
    "        train_op(\n",
    "            outdir=model_dir,\n",
    "            datadir=datasets_dir,\n",
    "            logdir=log_dir,\n",
    "            epochs=epochs,\n",
    "            bucket=bucket,\n",
    "            bucketdir=f\"{bucket_dir_tensorboard}/{model_name}/{model_version}/{dsl.RUN_ID_PLACEHOLDER}\",\n",
    "            metadatafile=metadata_file,\n",
    "        )\n",
    "        .add_pvolumes({mnt_path: download_task.pvolume})\n",
    "        .apply(aws_secret)\n",
    "    )\n",
    "    train_task.after(download_task)\n",
    "\n",
    "    evaluate_op = components.func_to_container_op(\n",
    "        evaluate_model,\n",
    "        base_image=TF_IMAGE,\n",
    "        output_component_file=\"evaluate_component.yaml\",\n",
    "    )\n",
    "    evaluate_task = evaluate_op(\n",
    "        metrics_path=metrics_path,\n",
    "        data_dir=datasets_dir,\n",
    "        model_dir=model_dir\n",
    "        # datasets_dir, modeldir=train_task.outputs[\"modeldir\"]\n",
    "    ).add_pvolumes({mnt_path: train_task.pvolume})\n",
    "    evaluate_task.after(train_task)\n",
    "    vop.delete().after(evaluate_task)\n",
    "\n",
    "    export_op = components.func_to_container_op(export_model, base_image=TF_IMAGE)\n",
    "    export_task = (\n",
    "        export_op(\n",
    "            # modeldir=train_task.outputs[\"modeldir\"],\n",
    "            src_dir=model_dir,\n",
    "            bucket_name=bucket,\n",
    "            bucket_dir=f\"{bucket_dir_model}/{model_name}/{model_version}\",\n",
    "        )\n",
    "        .add_pvolumes({mnt_path: evaluate_task.pvolume})\n",
    "        .apply(aws_secret)\n",
    "    )\n",
    "    export_task.after(evaluate_task)\n",
    "\n",
    "\n",
    "host = \"ml-pipeline.andrew.svc.cluster.local:8888\"\n",
    "experiment_name = \"mnist_notebook_server\"\n",
    "run_name = \"run01\"\n",
    "# namespace = \"kubeflow\"\n",
    "# namespace = \"andrew\"\n",
    "# client = Client(namespace=namespace)\n",
    "client = Client(host=host)\n",
    "# client = Client(namespace=namespace, host=host)\n",
    "arguments = {}\n",
    "run_result = client.create_run_from_pipeline_func(\n",
    "    mnist_pipeline,\n",
    "    experiment_name=experiment_name,\n",
    "    run_name=run_name,\n",
    "    arguments=arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "KF Fashion MNIST",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "9b101f8f6c39c3bc4991938ceb46b3b4b7491be66ceff76c9c3f27b191be2f59"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
